# -*- coding: utf-8 -*-
"""credit_kaggle

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_3T7vPGrtaxVs5bWNLGggvG6p5L2oct
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns

dataset_credit = pd.read_csv("https://github.com/ybifoundation/Dataset/raw/main/Credit%20Default.csv")

dataset_credit

dataset_credit.isnull().sum()

dataset_credit.info()

plt.plot(dataset_credit['Age'], marker ='o',linestyle = '-')

"""Vamos agora fazer uma análise breve do próximo passo dado que o dataset já está para submete-lo a algum modelo de classificação.
Foi observado que a base de dados é uma tanto pequena em relação a parametros de ánalise e elas têm relação direta entre si. Desse modo, podemos descartar modelos que consideram a idependência entre os parametros. Observando também que saída é binária podemos utilizar uma rede neural com uma função sigmoid na saída para a resolução de problema e de maneira mais completa fazer também uma regressão logistica de comparar os resultados.

**Utilização de rede neural com PyTorch**
"""

import torch.nn as nn
import torch

np.random.seed(123)
torch.manual_seed

x = dataset_credit.drop(columns = ['Default'])
y = dataset_credit['Default']

# Aqui é feita a divisão entre previsores e previsões

previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(x,y,test_size = 0.25)
# Divisão da base de dados entre treino (75%) e teste (25%)

# Quantidade de entradas = 4; quantidade de saídas = 1; (4+1)/2 = 2,5, a quantidade de neuronios será 3

classificador = nn.Sequential(
    nn.Linear(4,3),
    nn.ReLU(),
    nn.Linear(3,3),
    nn.ReLU(),
    nn.Linear(3,1),
    nn.Sigmoid()
)

criterion = nn.BCELoss() # Função para obter os erros da rede
optimizer = torch.optim.Adam(classificador.parameters(), lr = 0.001, weight_decay = 0.0001) # Função do ajuste de pesos da rede

"""Criada a rede neural, agora basta converter os datasets de treino para o formato de tensor, pois o pytorch requer esse formato."""

previsores_treinamento = torch.tensor(np.array(previsores_treinamento), dtype=torch.float)
classe_treinamento = torch.tensor(np.array(classe_treinamento), dtype = torch.float)

dataset_treinamento = torch.utils.data.TensorDataset(previsores_treinamento, classe_treinamento)

train_loader = torch.utils.data.DataLoader(dataset_treinamento, batch_size= 10, shuffle=True)

for epoch in range(100):
  #running_loss = 0.

  for data in train_loader:
    inputs, labels = data # Note que esses inputs são na verdade uma derivação do dos previsores treinamento
    optimizer.zero_grad() # Zera o gradiente acumulado

    outputs = classificador(inputs)
    loss = criterion(outputs.squeeze(), labels) # Nessa parte do código é onde é feita o calculo de erro onde recebe como parametros a sáida esperada(labels) e output da rede
    loss.backward() # Volta ao começo da rede
    optimizer.step() # Ajuste de pesos propriamente dito

classificador.eval() # Encerrando o modo de treimo e começando o modo de teste

previsores_teste = torch.tensor(np.array(previsores_teste), dtype=torch.float)

previsoes = classificador(previsores_teste)

previsoes

"""Como no dataset é usado 1 para a inadimplência e 0 para não inadiplencia, para um banco cujo o o objetivo é evitar tomar calotes e não necessariamente acertar em quem irá ser ou não inadimplente. Podemos ser um pouco mais conservadores e considerar somente aquele que estão mais perto do 0, não necessariamente 50%"""

previsoes = np.array(previsoes)
classe_teste = np.array(classe_teste)

classe_teste

previsoes = (previsoes > 0.4)

previsoes

acuracia = accuracy_score(classe_teste, previsoes)

acuracia

matriz = confusion_matrix(classe_teste, previsoes)

plt.figure(figsize=(5, 4))
sns.heatmap(matriz, annot=True, fmt='d', cmap='Blues', xticklabels=["Negativo", "Positivo"], yticklabels=["Negativo", "Positivo"])
plt.xlabel("Previsto")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

"""Com esse resultados ficou evidente que não ficou boa a forma da qual foi feita. A rede ficou 'viciada' em  em dar 0, ou proximo disso. Iremos agora experimentar outras abordagens.

A escolhida por mim será uma regressão com XGBoost
"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=2025) # Dividiremos do mesmo tamanho usado na NN

# Criar o modelo XGBoost para regressão
model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100, learning_rate=0.1, max_depth=3)

# Treinar o modelo
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Calcular o erro médio quadrático (MSE)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Valores Reais")
plt.ylabel("Previsões")
plt.title("Regressão com XGBoost")
plt.show()

"""Usando o XGBoost o erro foi praticamente para 0 !!!
O intuito do meu experimento era documentar e comparar o desempenho de uma rede neural simples com um modelo de classificação diferente.
A conclusão principal que podemos tirar desse pequeno experimento é que nem sempre uma Rede Neural vai ser a melhor saída para o seu problema. É necessária a avaliação de outros modelos de classificação antes de partir logo para uma NN.

"""